{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"RNNs.ipynb","provenance":[],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.0"}},"cells":[{"cell_type":"markdown","metadata":{"id":"QSMXqNHYhbLL","colab_type":"text"},"source":["# Задание 6: Рекуррентные нейронные сети (RNNs)\n","\n","Это задание адаптиповано из Deep NLP Course at ABBYY (https://github.com/DanAnastasyev/DeepNLP-Course) с разрешения автора - Даниила Анастасьева. Спасибо ему огромное!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P59NYU98GCb9","colab":{}},"source":["# !pip3 -qq install torch==0.4.1\n","# !pip3 -qq install bokeh==0.13.0\n","# !pip3 -qq install gensim==3.6.0\n","# !pip3 -qq install nltk\n","# !pip3 -qq install scikit-learn==0.20.2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8sVtGHmA9aBM","colab":{}},"source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","\n","if torch.cuda.is_available():\n","    from torch.cuda import FloatTensor, LongTensor\n","else:\n","    from torch import FloatTensor, LongTensor\n","\n","np.random.seed(42)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"-6CNKM3b4hT1"},"source":["# Рекуррентные нейронные сети (RNNs)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"O_XkoGNQUeGm"},"source":["## POS Tagging"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"QFEtWrS_4rUs"},"source":["Мы рассмотрим применение рекуррентных сетей к задаче sequence labeling (последняя картинка).\n","\n","![RNN types](http://karpathy.github.io/assets/rnn/diags.jpeg)\n","\n","*From [The Unreasonable Effectiveness of Recurrent Neural Networks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/)*\n","\n","Самые популярные примеры для такой постановки задачи - Part-of-Speech Tagging и Named Entity Recognition.\n","\n","Мы порешаем сейчас POS Tagging для английского.\n","\n","Будем работать с таким набором тегов:\n","- ADJ - adjective (new, good, high, ...)\n","- ADP - adposition (on, of, at, ...)\n","- ADV - adverb (really, already, still, ...)\n","- CONJ - conjunction (and, or, but, ...)\n","- DET - determiner, article (the, a, some, ...)\n","- NOUN - noun (year, home, costs, ...)\n","- NUM - numeral (twenty-four, fourth, 1991, ...)\n","- PRT - particle (at, on, out, ...)\n","- PRON - pronoun (he, their, her, ...)\n","- VERB - verb (is, say, told, ...)\n","- . - punctuation marks (. , ;)\n","- X - other (ersatz, esprit, dunno, ...)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"EPIkKdFlHB-X"},"source":["Скачаем данные:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TiA2dGmgF1rW","outputId":"2a2171a7-33c8-4d78-efbb-ffcab392e112","executionInfo":{"status":"ok","timestamp":1576314164506,"user_tz":-180,"elapsed":3604,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":84}},"source":["import nltk\n","from sklearn.model_selection import train_test_split\n","\n","nltk.download('brown')\n","nltk.download('universal_tagset')\n","\n","data = nltk.corpus.brown.tagged_sents(tagset='universal')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[nltk_data] Downloading package brown to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/brown.zip.\n","[nltk_data] Downloading package universal_tagset to /root/nltk_data...\n","[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"d93g_swyJA_V"},"source":["Пример размеченного предложения:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"QstS4NO0L97c","outputId":"b6c6cde4-5d13-4555-f375-5ac7c77b8fd1","executionInfo":{"status":"ok","timestamp":1576314164507,"user_tz":-180,"elapsed":3595,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":437}},"source":["for word, tag in data[0]:\n","    print('{:15}\\t{}'.format(word, tag))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["The            \tDET\n","Fulton         \tNOUN\n","County         \tNOUN\n","Grand          \tADJ\n","Jury           \tNOUN\n","said           \tVERB\n","Friday         \tNOUN\n","an             \tDET\n","investigation  \tNOUN\n","of             \tADP\n","Atlanta's      \tNOUN\n","recent         \tADJ\n","primary        \tNOUN\n","election       \tNOUN\n","produced       \tVERB\n","``             \t.\n","no             \tDET\n","evidence       \tNOUN\n","''             \t.\n","that           \tADP\n","any            \tDET\n","irregularities \tNOUN\n","took           \tVERB\n","place          \tNOUN\n",".              \t.\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"epdW8u_YXcAv"},"source":["Построим разбиение на train/val/test - наконец-то, всё как у нормальных людей.\n","\n","На train будем учиться, по val - подбирать параметры и делать всякие early stopping, а на test - принимать модель по ее финальному качеству."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"xTai8Ta0lgwL","outputId":"09648edd-6a26-4da9-dde4-003954c1a5e3","executionInfo":{"status":"ok","timestamp":1576314191256,"user_tz":-180,"elapsed":30335,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":67}},"source":["train_data, test_data = train_test_split(data, test_size=0.25, random_state=42)\n","train_data, val_data = train_test_split(train_data, test_size=0.15, random_state=42)\n","\n","print('Words count in train set:', sum(len(sent) for sent in train_data))\n","print('Words count in val set:', sum(len(sent) for sent in val_data))\n","print('Words count in test set:', sum(len(sent) for sent in test_data))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Words count in train set: 739769\n","Words count in val set: 130954\n","Words count in test set: 290469\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"eChdLNGtXyP0"},"source":["Построим маппинги из слов в индекс и из тега в индекс:\n"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"pCjwwDs6Zq9x","outputId":"c02261f3-8308-45af-e2c2-0fc776bbb7ed","executionInfo":{"status":"ok","timestamp":1576314191497,"user_tz":-180,"elapsed":30563,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["words = {word for sample in train_data for word, tag in sample}\n","word2ind = {word: ind + 1 for ind, word in enumerate(words)}\n","word2ind['<pad>'] = 0\n","\n","tags = {tag for sample in train_data for word, tag in sample}\n","tag2ind = {tag: ind + 1 for ind, tag in enumerate(tags)}\n","tag2ind['<pad>'] = 0\n","\n","print('Unique words in train = {}. Tags = {}'.format(len(word2ind), tags))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Unique words in train = 45441. Tags = {'X', 'ADP', 'ADV', 'PRON', 'ADJ', 'DET', 'CONJ', 'VERB', 'NOUN', 'PRT', 'NUM', '.'}\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"URC1B2nvPGFt","outputId":"cd130c36-c607-4be1-c873-9b77d17493a6","executionInfo":{"status":"ok","timestamp":1576314192313,"user_tz":-180,"elapsed":31366,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":320}},"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","from collections import Counter\n","\n","tag_distribution = Counter(tag for sample in train_data for _, tag in sample)\n","tag_distribution = [tag_distribution[tag] for tag in tags]\n","\n","plt.figure(figsize=(10, 5))\n","\n","bar_width = 0.35\n","plt.bar(np.arange(len(tags)), tag_distribution, bar_width, align='center', alpha=0.5)\n","plt.xticks(np.arange(len(tags)), tags)\n","    \n","plt.show()"],"execution_count":0,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAmkAAAEvCAYAAAAemFY+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nUocMaZIkSR0ypEmSJHXo/wO6sYV1oGVnFgAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 720x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"gArQwbzWWkgi"},"source":["## Бейзлайн\n","\n","Какой самый простой теггер можно придумать? Давайте просто запоминать, какие теги самые вероятные для слова (или для последовательности):\n","\n","![tag-context](https://www.nltk.org/images/tag-context.png)  \n","*From [Categorizing and Tagging Words, nltk](https://www.nltk.org/book/ch05.html)*\n","\n","На картинке показано, что для предсказания $t_n$ используются два предыдущих предсказанных тега + текущее слово. По корпусу считаются вероятность для $P(t_n| w_n, t_{n-1}, t_{n-2})$, выбирается тег с максимальной вероятностью.\n","\n","Более аккуратно такая идея реализована в Hidden Markov Models: по тренировочному корпусу вычисляются вероятности $P(w_n| t_n), P(t_n|t_{n-1}, t_{n-2})$ и максимизируется их произведение.\n","\n","Простейший вариант - униграммная модель, учитывающая только слово:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"5rWmSToIaeAo","outputId":"b4bb5038-82e1-483b-e757-67d636d64770","executionInfo":{"status":"ok","timestamp":1576314195440,"user_tz":-180,"elapsed":34479,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import nltk\n","\n","default_tagger = nltk.DefaultTagger('NN')\n","\n","unigram_tagger = nltk.UnigramTagger(train_data, backoff=default_tagger)\n","print('Accuracy of unigram tagger = {:.2%}'.format(unigram_tagger.evaluate(test_data)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of unigram tagger = 92.62%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"07Ymb_MkbWsF"},"source":["Добавим вероятности переходов:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"vjz_Rk0bbMyH","outputId":"1a69027d-4dd3-45be-a834-e904fc3828a4","executionInfo":{"status":"ok","timestamp":1576314200107,"user_tz":-180,"elapsed":39139,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["bigram_tagger = nltk.BigramTagger(train_data, backoff=unigram_tagger)\n","print('Accuracy of bigram tagger = {:.2%}'.format(bigram_tagger.evaluate(test_data)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of bigram tagger = 93.42%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"uWMw6QHvbaDd"},"source":["Обратите внимание, что `backoff` важен:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"8XCuxEBVbOY_","outputId":"2fd626bd-55e0-4e49-e14c-4911d1ad54a8","executionInfo":{"status":"ok","timestamp":1576314204839,"user_tz":-180,"elapsed":43862,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["trigram_tagger = nltk.TrigramTagger(train_data)\n","print('Accuracy of trigram tagger = {:.2%}'.format(trigram_tagger.evaluate(test_data)))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy of trigram tagger = 23.33%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"4t3xyYd__8d-"},"source":["## Увеличиваем контекст с рекуррентными сетями\n","\n","Униграмная модель работает на удивление хорошо, но мы же собрались учить сеточки.\n","\n","Омонимия - основная причина, почему униграмная модель плоха:  \n","*“he cashed a check at the **bank**”*  \n","vs  \n","*“he sat on the **bank** of the river”*\n","\n","Поэтому нам очень полезно учитывать контекст при предсказании тега.\n","\n","Воспользуемся LSTM - он умеет работать с контекстом очень даже хорошо:\n","\n","![](https://image.ibb.co/kgmoff/Baseline-Tagger.png)\n","\n","Синим показано выделение фичей из слова, LSTM оранжевенький - он строит эмбеддинги слов с учетом контекста, а дальше зелененькая логистическая регрессия делает предсказания тегов."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"RtRbz1SwgEqc","colab":{}},"source":["def convert_data(data, word2ind, tag2ind):\n","    X = [[word2ind.get(word, 0) for word, _ in sample] for sample in data]\n","    y = [[tag2ind[tag] for _, tag in sample] for sample in data]\n","    \n","    return X, y\n","\n","X_train, y_train = convert_data(train_data, word2ind, tag2ind)\n","X_val, y_val = convert_data(val_data, word2ind, tag2ind)\n","X_test, y_test = convert_data(test_data, word2ind, tag2ind)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DhsTKZalfih6","colab":{}},"source":["def iterate_batches(data, batch_size):\n","    X, y = data\n","    n_samples = len(X)\n","\n","    indices = np.arange(n_samples)\n","    np.random.shuffle(indices)\n","    \n","    for start in range(0, n_samples, batch_size):\n","        end = min(start + batch_size, n_samples)\n","        \n","        batch_indices = indices[start:end]\n","        \n","        max_sent_len = max(len(X[ind]) for ind in batch_indices)\n","        X_batch = np.zeros((max_sent_len, len(batch_indices)))\n","        y_batch = np.zeros((max_sent_len, len(batch_indices)))\n","        \n","        for batch_ind, sample_ind in enumerate(batch_indices):\n","            X_batch[:len(X[sample_ind]), batch_ind] = X[sample_ind]\n","            y_batch[:len(y[sample_ind]), batch_ind] = y[sample_ind]\n","            \n","        yield X_batch, y_batch"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","outputId":"2f4e66f4-a068-4d34-bbc8-3e2b4d40dc44","executionInfo":{"status":"ok","timestamp":1576314206049,"user_tz":-180,"elapsed":45052,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"id":"GpUJFpMyuMLS","colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_batch, y_batch = next(iterate_batches((X_train, y_train), 4))\n","\n","X_batch.shape, y_batch.shape"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["((32, 4), (32, 4))"]},"metadata":{"tags":[]},"execution_count":13}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"C5I9E9P6eFYv"},"source":["**Задание** Реализуйте `LSTMTagger`:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WVEHju54d68T","colab":{}},"source":["class LSTMTagger(nn.Module):\n","    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n","        self.lstm = nn.LSTM(input_size = word_emb_dim, \n","                            hidden_size = lstm_hidden_dim,\n","                            num_layers = lstm_layers_count,\n","                            bias = True)\n","        \n","        self.linear = nn.Linear(lstm_hidden_dim, tagset_size, bias=False)\n","\n","    def forward(self, inputs):\n","        \n","        embedding_out = self.embedding(inputs)\n","        \n","        lstm_out, _ = self.lstm.forward(embedding_out)\n","        linear_out = self.linear.forward(lstm_out)\n","        \n","        return linear_out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"q_HA8zyheYGH"},"source":["**Задание** Научитесь считать accuracy и loss (а заодно проверьте, что модель работает)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"jbrxsZ2mehWB","colab":{}},"source":["model = LSTMTagger(\n","    vocab_size=len(word2ind),\n","    tagset_size=len(tag2ind)\n",")"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tx6JzWXTwbL4","colab_type":"code","outputId":"40fe8e3c-0442-434c-b209-9a4d2173adc7","executionInfo":{"status":"ok","timestamp":1576314206051,"user_tz":-180,"elapsed":45030,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["X_batch, y_batch = torch.LongTensor(X_batch), torch.LongTensor(y_batch)\n","\n","logits = model(X_batch)\n","\n","indices_log = torch.max(logits, dim=2)[1]\n","\n","correct_samples = float(torch.sum(indices_log[y_batch!=0] == y_batch[y_batch!=0]))\n","print('accuracy = {:.3f}'.format(correct_samples / (y_batch[y_batch!=0].shape[0])))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["accuracy = 0.054\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"GMUyUm1hgpe3","outputId":"4ea0fddf-5531-4d44-bbca-5a14861d9182","executionInfo":{"status":"ok","timestamp":1576314206051,"user_tz":-180,"elapsed":45022,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["criterion = nn.CrossEntropyLoss(ignore_index=0)\n","test = logits.type(torch.FloatTensor)\n","test_shaped = test.view(-1, 13)\n","y = y_batch.view(-1)\n","loss1 = criterion(test_shaped, y)\n","print(loss1)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(2.5778, grad_fn=<NllLossBackward>)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"p48aHvx2wjH2","colab_type":"code","outputId":"56f84614-be9f-48cc-b1c8-2f297bad6115","executionInfo":{"status":"ok","timestamp":1576314206299,"user_tz":-180,"elapsed":45253,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["loss2 = 0\n","for batch in range(y_batch.shape[1]):\n","  loss2 += criterion(test[:, batch, :], y_batch[:, batch])\n","\n","print(loss2 / (batch+1))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["tensor(2.5803, grad_fn=<DivBackward0>)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nSgV3NPUpcjH"},"source":["**Задание** Вставьте эти вычисление в функцию:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"FprPQ0gllo7b","colab":{}},"source":["import math\n","from tqdm import tqdm\n","\n","\n","def do_epoch(model, criterion, data, batch_size, optimizer=None, name=None):\n","    epoch_loss = 0\n","    correct_count = 0\n","    sum_count = 0\n","    \n","    is_train = not optimizer is None\n","    name = name or ''\n","    model.train(is_train)\n","    \n","    batches_count = math.ceil(len(data[0]) / batch_size)\n","    \n","    with torch.autograd.set_grad_enabled(is_train):\n","        with tqdm(total=batches_count) as progress_bar:\n","            for i, (X_batch, y_batch) in enumerate(iterate_batches(data, batch_size)):\n","                X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n","                \n","                logits = model(X_batch)\n","#                 criterion = nn.CrossEntropyLoss(ignore_index=0)\n","                \n","                indices = torch.max(logits, dim=2)[1]\n","                \n","                loss = criterion(logits.view(-1, 13), y_batch.view(-1))\n","\n","                epoch_loss += loss.item()\n","\n","                if optimizer:\n","                    optimizer.zero_grad()\n","                    loss.backward()\n","                    optimizer.step()\n","                    \n","                indices = torch.max(logits, dim=2)[1]   \n","                correct_samples = float(torch.sum(indices[y_batch!=0] == y_batch[y_batch!=0]))\n","                \n","                cur_correct_count, cur_sum_count = correct_samples, y_batch[y_batch!=0].shape[0]\n","\n","                correct_count += cur_correct_count\n","                sum_count += cur_sum_count\n","\n","                progress_bar.update()\n","                progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n","                    name, loss.item(), cur_correct_count / cur_sum_count)\n","                )\n","                \n","            progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n","                name, epoch_loss / batches_count, correct_count / sum_count)\n","            )\n","\n","    return epoch_loss / batches_count, correct_count / sum_count\n","\n","\n","def fit(model, criterion, optimizer, train_data, epochs_count=1, batch_size=32,\n","        val_data=None, val_batch_size=None):\n","        \n","    if not val_data is None and val_batch_size is None:\n","        val_batch_size = batch_size\n","        \n","    for epoch in range(epochs_count):\n","        name_prefix = '[{} / {}] '.format(epoch + 1, epochs_count)\n","        train_loss, train_acc = do_epoch(model, criterion, train_data, batch_size, optimizer, name_prefix + 'Train:')\n","        \n","        if not val_data is None:\n","            val_loss, val_acc = do_epoch(model, criterion, val_data, val_batch_size, None, name_prefix + '  Val:')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Pqfbeh1ltEYa","outputId":"17c457a7-feb1-4369-812b-0c5b88c1ec28","executionInfo":{"status":"ok","timestamp":1576314298200,"user_tz":-180,"elapsed":137134,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":521}},"source":["model = LSTMTagger(\n","    vocab_size=len(word2ind),\n","    tagset_size=len(tag2ind)\n",").cuda()\n","\n","criterion = nn.CrossEntropyLoss(ignore_index=0).cuda()\n","optimizer = optim.Adam(model.parameters())\n","\n","fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=15,\n","    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1 / 15] Train: Loss = 0.68873, Accuracy = 78.38%: 100%|██████████| 572/572 [00:05<00:00, 102.26it/s]\n","[1 / 15]   Val: Loss = 0.37465, Accuracy = 86.94%: 100%|██████████| 13/13 [00:00<00:00, 78.89it/s]\n","[2 / 15] Train: Loss = 0.27528, Accuracy = 90.86%: 100%|██████████| 572/572 [00:05<00:00, 102.44it/s]\n","[2 / 15]   Val: Loss = 0.25668, Accuracy = 91.04%: 100%|██████████| 13/13 [00:00<00:00, 81.27it/s]\n","[3 / 15] Train: Loss = 0.18582, Accuracy = 93.87%: 100%|██████████| 572/572 [00:05<00:00, 102.83it/s]\n","[3 / 15]   Val: Loss = 0.21586, Accuracy = 92.57%: 100%|██████████| 13/13 [00:00<00:00, 79.74it/s]\n","[4 / 15] Train: Loss = 0.13882, Accuracy = 95.38%: 100%|██████████| 572/572 [00:05<00:00, 100.42it/s]\n","[4 / 15]   Val: Loss = 0.20097, Accuracy = 93.34%: 100%|██████████| 13/13 [00:00<00:00, 82.19it/s]\n","[5 / 15] Train: Loss = 0.10791, Accuracy = 96.41%: 100%|██████████| 572/572 [00:05<00:00, 103.93it/s]\n","[5 / 15]   Val: Loss = 0.18485, Accuracy = 93.87%: 100%|██████████| 13/13 [00:00<00:00, 76.13it/s]\n","[6 / 15] Train: Loss = 0.08596, Accuracy = 97.11%: 100%|██████████| 572/572 [00:05<00:00, 103.94it/s]\n","[6 / 15]   Val: Loss = 0.18353, Accuracy = 94.14%: 100%|██████████| 13/13 [00:00<00:00, 78.92it/s]\n","[7 / 15] Train: Loss = 0.06914, Accuracy = 97.69%: 100%|██████████| 572/572 [00:05<00:00, 102.69it/s]\n","[7 / 15]   Val: Loss = 0.19033, Accuracy = 94.25%: 100%|██████████| 13/13 [00:00<00:00, 79.81it/s]\n","[8 / 15] Train: Loss = 0.05599, Accuracy = 98.13%: 100%|██████████| 572/572 [00:05<00:00, 102.53it/s]\n","[8 / 15]   Val: Loss = 0.20173, Accuracy = 94.31%: 100%|██████████| 13/13 [00:00<00:00, 75.01it/s]\n","[9 / 15] Train: Loss = 0.04575, Accuracy = 98.48%: 100%|██████████| 572/572 [00:05<00:00, 102.55it/s]\n","[9 / 15]   Val: Loss = 0.20459, Accuracy = 94.33%: 100%|██████████| 13/13 [00:00<00:00, 77.63it/s]\n","[10 / 15] Train: Loss = 0.03733, Accuracy = 98.77%: 100%|██████████| 572/572 [00:05<00:00, 103.09it/s]\n","[10 / 15]   Val: Loss = 0.22016, Accuracy = 94.31%: 100%|██████████| 13/13 [00:00<00:00, 82.39it/s]\n","[11 / 15] Train: Loss = 0.03050, Accuracy = 99.02%: 100%|██████████| 572/572 [00:05<00:00, 102.80it/s]\n","[11 / 15]   Val: Loss = 0.21824, Accuracy = 94.27%: 100%|██████████| 13/13 [00:00<00:00, 75.01it/s]\n","[12 / 15] Train: Loss = 0.02502, Accuracy = 99.19%: 100%|██████████| 572/572 [00:05<00:00, 104.85it/s]\n","[12 / 15]   Val: Loss = 0.23629, Accuracy = 94.32%: 100%|██████████| 13/13 [00:00<00:00, 80.89it/s]\n","[13 / 15] Train: Loss = 0.02034, Accuracy = 99.36%: 100%|██████████| 572/572 [00:05<00:00, 102.47it/s]\n","[13 / 15]   Val: Loss = 0.25150, Accuracy = 94.24%: 100%|██████████| 13/13 [00:00<00:00, 76.84it/s]\n","[14 / 15] Train: Loss = 0.01645, Accuracy = 99.50%: 100%|██████████| 572/572 [00:05<00:00, 103.30it/s]\n","[14 / 15]   Val: Loss = 0.26518, Accuracy = 94.21%: 100%|██████████| 13/13 [00:00<00:00, 73.79it/s]\n","[15 / 15] Train: Loss = 0.01367, Accuracy = 99.58%: 100%|██████████| 572/572 [00:05<00:00, 104.00it/s]\n","[15 / 15]   Val: Loss = 0.27910, Accuracy = 94.23%: 100%|██████████| 13/13 [00:00<00:00, 77.03it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"m0qGetIhfUE5"},"source":["### Masking\n","\n","**Задание** Проверьте себя - не считаете ли вы потери и accuracy на паддингах - очень легко получить высокое качество за счет этого.\n","\n","У функции потерь есть параметр `ignore_index`, для таких целей. Для accuracy нужно использовать маскинг - умножение на маску из нулей и единиц, где нули на позициях паддингов (а потом усреднение по ненулевым позициям в маске)."]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"nAfV2dEOfHo5"},"source":["**Задание** Посчитайте качество модели на тесте. Ожидается результат лучше бейзлайна!"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"98wr38_rw55D","outputId":"56983187-0f6e-4358-baa1-7ed9d3873534","executionInfo":{"status":"ok","timestamp":1576314298754,"user_tz":-180,"elapsed":137678,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["batch_size = len(X_test)\n","name = 'Test'\n","epoch_loss = 0\n","correct_count = 0\n","sum_count = 0\n","model.eval()\n","batches_count = math.ceil(len(X_test) / batch_size)\n","with tqdm(total=batches_count) as progress_bar:\n","  for i, (X_batch, y_batch) in enumerate(iterate_batches((X_test, y_test), batch_size)):\n","    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n","\n","    logits = model(X_batch)\n","\n","    indices = torch.max(logits, dim=2)[1]\n","\n","    loss = criterion(logits.view(-1, 13), y_batch.view(-1))\n","\n","    epoch_loss += loss.item()\n","\n","  #   if optimizer:\n","  #       optimizer.zero_grad()\n","  #       loss.backward()\n","  #       optimizer.step()\n","\n","    indices = torch.max(logits, dim=2)[1]   \n","    correct_samples = float(torch.sum(indices[y_batch!=0] == y_batch[y_batch!=0]))\n","\n","    cur_correct_count, cur_sum_count = correct_samples, y_batch[y_batch!=0].shape[0]\n","\n","    correct_count += cur_correct_count\n","    sum_count += cur_sum_count\n","\n","    progress_bar.update()\n","    progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n","        name, loss.item(), cur_correct_count / cur_sum_count)\n","    )\n","progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n","    name, epoch_loss / batches_count, correct_count / sum_count)\n","                            )"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" Test Loss = 0.27899, Accuracy = 94.29%: 100%|██████████| 1/1 [00:00<00:00,  3.21it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"PXUTSFaEHbDG"},"source":["### Bidirectional LSTM\n","\n","Благодаря BiLSTM можно использовать сразу оба контеста при предсказании тега слова. Т.е. для каждого токена $w_i$ forward LSTM будет выдавать представление $\\mathbf{f_i} \\sim (w_1, \\ldots, w_i)$ - построенное по всему левому контексту - и $\\mathbf{b_i} \\sim (w_n, \\ldots, w_i)$ - представление правого контекста. Их конкатенация автоматически захватит весь доступный контекст слова: $\\mathbf{h_i} = [\\mathbf{f_i}, \\mathbf{b_i}] \\sim (w_1, \\ldots, w_n)$.\n","\n","![BiLSTM](https://www.researchgate.net/profile/Wang_Ling/publication/280912217/figure/fig2/AS:391505383575555@1470353565299/Illustration-of-our-neural-network-for-POS-tagging.png)  \n","*From [Finding Function in Form: Compositional Character Models for Open Vocabulary Word Representation](https://arxiv.org/abs/1508.02096)*\n","\n","**Задание** Добавьте Bidirectional LSTM."]},{"cell_type":"code","metadata":{"id":"Fyg0_K13hbMH","colab_type":"code","colab":{}},"source":["class BiLSTMTagger(nn.Module):\n","    def __init__(self, vocab_size, tagset_size, word_emb_dim=100, lstm_hidden_dim=128, lstm_layers_count=1):\n","        super().__init__()\n","        \n","        self.embedding = nn.Embedding(vocab_size, word_emb_dim)\n","        self.lstm = nn.LSTM(input_size = word_emb_dim, \n","                            hidden_size = lstm_hidden_dim,\n","                            num_layers = lstm_layers_count,\n","                            bias = True,\n","                            bidirectional = True)\n","        \n","        self.linear = nn.Linear(lstm_hidden_dim, tagset_size, bias=False)\n","\n","    def forward(self, inputs):\n","        \n","        embedding_out = self.embedding(inputs)\n","        \n","        lstm_out, _ = self.lstm.forward(embedding_out)\n","        linear_out = self.linear.forward(lstm_out)\n","        \n","        return linear_out"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"ZTXmYGD_ANhm"},"source":["### Предобученные эмбеддинги\n","\n","Мы знаем, какая клёвая вещь - предобученные эмбеддинги. При текущем размере обучающей выборки еще можно было учить их и с нуля - с меньшей было бы совсем плохо.\n","\n","Поэтому стандартный пайплайн - скачать эмбеддинги, засунуть их в сеточку. Запустим его:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"uZpY_Q1xZ18h","outputId":"4fa77156-5230-4e66-bd54-89eb411706cd","executionInfo":{"status":"ok","timestamp":1576314369182,"user_tz":-180,"elapsed":208093,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":87}},"source":["import gensim.downloader as api\n","\n","w2v_model = api.load('glove-wiki-gigaword-100')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[==================================================] 100.0% 128.1/128.1MB downloaded\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/smart_open/smart_open_lib.py:402: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n","  'See the migration notes for details: %s' % _MIGRATION_NOTES_URL\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"KYogOoKlgtcf"},"source":["Построим подматрицу для слов из нашей тренировочной выборки:"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"VsCstxiO03oT","outputId":"35eeca78-1eb8-4653-f190-b89d6cbf347e","executionInfo":{"status":"ok","timestamp":1576314554826,"user_tz":-180,"elapsed":644,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":50}},"source":["known_count = 0\n","embeddings = np.zeros((len(word2ind), w2v_model.vectors.shape[1]))\n","for word, ind in word2ind.items():\n","    word = word.lower()\n","    if word in w2v_model.vocab:\n","        embeddings[ind] = w2v_model.get_vector(word)\n","        known_count += 1\n","        \n","print('Know {} out of {} word embeddings'.format(known_count, len(word2ind)))\n","print(embeddings.shape)\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Know 38736 out of 45441 word embeddings\n","(45441, 100)\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"HcG7i-R8hbY3"},"source":["**Задание** Сделайте модель с предобученной матрицей. Используйте `nn.Embedding.from_pretrained`."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"LxaRBpQd0pat","colab":{}},"source":["class LSTMTaggerWithPretrainedEmbs(nn.Module):\n","    def __init__(self, embeddings, tagset_size, lstm_hidden_dim=64, lstm_layers_count=1):\n","        super().__init__()\n","        self.embed = nn.Embedding.from_pretrained(embeddings=FloatTensor(embeddings))\n","        self.lstm = nn.LSTM(input_size=embeddings.shape[1],\n","                           hidden_size=lstm_hidden_dim,\n","                           num_layers=lstm_layers_count,\n","                           bidirectional=True)\n","        self.linear = nn.Linear(lstm_hidden_dim*2, tagset_size)\n","    def forward(self, inputs):\n","        \n","        embed_out = self.embed(inputs)\n","        lstm_out, *_ = self.lstm(embed_out)\n","        linear_out = self.linear(lstm_out)\n","        return linear_out"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EBtI6BDE-Fc7","outputId":"9c2efb26-2efb-40f0-e843-1371ce0ab03f","executionInfo":{"status":"ok","timestamp":1576317396990,"user_tz":-180,"elapsed":348395,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["model = LSTMTaggerWithPretrainedEmbs(\n","    embeddings=embeddings,\n","    tagset_size=len(tag2ind)\n",").cuda()\n","criterion = nn.CrossEntropyLoss(ignore_index=0)\n","optimizer = optim.Adam(model.parameters())\n","\n","fit(model, criterion, optimizer, train_data=(X_train, y_train), epochs_count=50,\n","    batch_size=64, val_data=(X_val, y_val), val_batch_size=512)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[1 / 50] Train: Loss = 0.56594, Accuracy = 83.73%: 100%|██████████| 572/572 [00:06<00:00, 84.86it/s]\n","[1 / 50]   Val: Loss = 0.25470, Accuracy = 92.54%: 100%|██████████| 13/13 [00:00<00:00, 74.86it/s]\n","[2 / 50] Train: Loss = 0.18820, Accuracy = 94.44%: 100%|██████████| 572/572 [00:06<00:00, 83.26it/s]\n","[2 / 50]   Val: Loss = 0.17357, Accuracy = 94.83%: 100%|██████████| 13/13 [00:00<00:00, 72.53it/s]\n","[3 / 50] Train: Loss = 0.13379, Accuracy = 96.03%: 100%|██████████| 572/572 [00:06<00:00, 84.64it/s]\n","[3 / 50]   Val: Loss = 0.14097, Accuracy = 95.66%: 100%|██████████| 13/13 [00:00<00:00, 76.55it/s]\n","[4 / 50] Train: Loss = 0.10781, Accuracy = 96.76%: 100%|██████████| 572/572 [00:06<00:00, 85.64it/s]\n","[4 / 50]   Val: Loss = 0.12271, Accuracy = 96.16%: 100%|██████████| 13/13 [00:00<00:00, 85.24it/s]\n","[5 / 50] Train: Loss = 0.09249, Accuracy = 97.19%: 100%|██████████| 572/572 [00:06<00:00, 84.69it/s]\n","[5 / 50]   Val: Loss = 0.11316, Accuracy = 96.47%: 100%|██████████| 13/13 [00:00<00:00, 80.97it/s]\n","[6 / 50] Train: Loss = 0.08249, Accuracy = 97.47%: 100%|██████████| 572/572 [00:06<00:00, 84.70it/s]\n","[6 / 50]   Val: Loss = 0.10718, Accuracy = 96.64%: 100%|██████████| 13/13 [00:00<00:00, 77.31it/s]\n","[7 / 50] Train: Loss = 0.07487, Accuracy = 97.68%: 100%|██████████| 572/572 [00:07<00:00, 81.65it/s]\n","[7 / 50]   Val: Loss = 0.10166, Accuracy = 96.81%: 100%|██████████| 13/13 [00:00<00:00, 82.30it/s]\n","[8 / 50] Train: Loss = 0.06884, Accuracy = 97.87%: 100%|██████████| 572/572 [00:06<00:00, 84.78it/s]\n","[8 / 50]   Val: Loss = 0.09936, Accuracy = 96.86%: 100%|██████████| 13/13 [00:00<00:00, 71.09it/s]\n","[9 / 50] Train: Loss = 0.06409, Accuracy = 98.02%: 100%|██████████| 572/572 [00:06<00:00, 84.42it/s]\n","[9 / 50]   Val: Loss = 0.09739, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 78.79it/s]\n","[10 / 50] Train: Loss = 0.05971, Accuracy = 98.13%: 100%|██████████| 572/572 [00:06<00:00, 83.70it/s]\n","[10 / 50]   Val: Loss = 0.09686, Accuracy = 96.90%: 100%|██████████| 13/13 [00:00<00:00, 77.88it/s]\n","[11 / 50] Train: Loss = 0.05635, Accuracy = 98.25%: 100%|██████████| 572/572 [00:06<00:00, 84.34it/s]\n","[11 / 50]   Val: Loss = 0.09578, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 77.04it/s]\n","[12 / 50] Train: Loss = 0.05302, Accuracy = 98.35%: 100%|██████████| 572/572 [00:06<00:00, 84.46it/s]\n","[12 / 50]   Val: Loss = 0.09325, Accuracy = 97.02%: 100%|██████████| 13/13 [00:00<00:00, 75.86it/s]\n","[13 / 50] Train: Loss = 0.05014, Accuracy = 98.43%: 100%|██████████| 572/572 [00:06<00:00, 84.97it/s]\n","[13 / 50]   Val: Loss = 0.09400, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 75.78it/s]\n","[14 / 50] Train: Loss = 0.04784, Accuracy = 98.50%: 100%|██████████| 572/572 [00:06<00:00, 84.56it/s]\n","[14 / 50]   Val: Loss = 0.09345, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 81.30it/s]\n","[15 / 50] Train: Loss = 0.04537, Accuracy = 98.58%: 100%|██████████| 572/572 [00:06<00:00, 83.62it/s]\n","[15 / 50]   Val: Loss = 0.09236, Accuracy = 97.07%: 100%|██████████| 13/13 [00:00<00:00, 76.39it/s]\n","[16 / 50] Train: Loss = 0.04320, Accuracy = 98.64%: 100%|██████████| 572/572 [00:06<00:00, 86.24it/s]\n","[16 / 50]   Val: Loss = 0.09424, Accuracy = 97.03%: 100%|██████████| 13/13 [00:00<00:00, 74.79it/s]\n","[17 / 50] Train: Loss = 0.04113, Accuracy = 98.71%: 100%|██████████| 572/572 [00:06<00:00, 84.46it/s]\n","[17 / 50]   Val: Loss = 0.09686, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 79.71it/s]\n","[18 / 50] Train: Loss = 0.03910, Accuracy = 98.77%: 100%|██████████| 572/572 [00:06<00:00, 85.61it/s]\n","[18 / 50]   Val: Loss = 0.09672, Accuracy = 97.03%: 100%|██████████| 13/13 [00:00<00:00, 75.50it/s]\n","[19 / 50] Train: Loss = 0.03748, Accuracy = 98.82%: 100%|██████████| 572/572 [00:06<00:00, 82.82it/s]\n","[19 / 50]   Val: Loss = 0.09409, Accuracy = 97.11%: 100%|██████████| 13/13 [00:00<00:00, 75.07it/s]\n","[20 / 50] Train: Loss = 0.03587, Accuracy = 98.88%: 100%|██████████| 572/572 [00:06<00:00, 84.58it/s]\n","[20 / 50]   Val: Loss = 0.09563, Accuracy = 97.04%: 100%|██████████| 13/13 [00:00<00:00, 78.06it/s]\n","[21 / 50] Train: Loss = 0.03420, Accuracy = 98.94%: 100%|██████████| 572/572 [00:06<00:00, 83.71it/s]\n","[21 / 50]   Val: Loss = 0.09624, Accuracy = 97.07%: 100%|██████████| 13/13 [00:00<00:00, 82.12it/s]\n","[22 / 50] Train: Loss = 0.03265, Accuracy = 99.00%: 100%|██████████| 572/572 [00:06<00:00, 84.40it/s]\n","[22 / 50]   Val: Loss = 0.09754, Accuracy = 97.07%: 100%|██████████| 13/13 [00:00<00:00, 75.52it/s]\n","[23 / 50] Train: Loss = 0.03120, Accuracy = 99.04%: 100%|██████████| 572/572 [00:06<00:00, 83.48it/s]\n","[23 / 50]   Val: Loss = 0.10155, Accuracy = 96.99%: 100%|██████████| 13/13 [00:00<00:00, 75.70it/s]\n","[24 / 50] Train: Loss = 0.02988, Accuracy = 99.08%: 100%|██████████| 572/572 [00:06<00:00, 84.14it/s]\n","[24 / 50]   Val: Loss = 0.10094, Accuracy = 97.05%: 100%|██████████| 13/13 [00:00<00:00, 79.97it/s]\n","[25 / 50] Train: Loss = 0.02846, Accuracy = 99.13%: 100%|██████████| 572/572 [00:06<00:00, 84.95it/s]\n","[25 / 50]   Val: Loss = 0.10296, Accuracy = 96.96%: 100%|██████████| 13/13 [00:00<00:00, 75.34it/s]\n","[26 / 50] Train: Loss = 0.02714, Accuracy = 99.18%: 100%|██████████| 572/572 [00:06<00:00, 85.16it/s] \n","[26 / 50]   Val: Loss = 0.10581, Accuracy = 97.01%: 100%|██████████| 13/13 [00:00<00:00, 83.60it/s]\n","[27 / 50] Train: Loss = 0.02591, Accuracy = 99.22%: 100%|██████████| 572/572 [00:06<00:00, 83.84it/s]\n","[27 / 50]   Val: Loss = 0.10466, Accuracy = 96.94%: 100%|██████████| 13/13 [00:00<00:00, 76.00it/s]\n","[28 / 50] Train: Loss = 0.02478, Accuracy = 99.26%: 100%|██████████| 572/572 [00:06<00:00, 84.11it/s]\n","[28 / 50]   Val: Loss = 0.10662, Accuracy = 97.00%: 100%|██████████| 13/13 [00:00<00:00, 74.89it/s]\n","[29 / 50] Train: Loss = 0.02360, Accuracy = 99.30%: 100%|██████████| 572/572 [00:06<00:00, 84.47it/s]\n","[29 / 50]   Val: Loss = 0.10966, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 77.64it/s]\n","[30 / 50] Train: Loss = 0.02260, Accuracy = 99.33%: 100%|██████████| 572/572 [00:06<00:00, 85.48it/s]\n","[30 / 50]   Val: Loss = 0.11027, Accuracy = 96.97%: 100%|██████████| 13/13 [00:00<00:00, 78.46it/s]\n","[31 / 50] Train: Loss = 0.02146, Accuracy = 99.37%: 100%|██████████| 572/572 [00:06<00:00, 84.03it/s]\n","[31 / 50]   Val: Loss = 0.11385, Accuracy = 96.88%: 100%|██████████| 13/13 [00:00<00:00, 76.92it/s]\n","[32 / 50] Train: Loss = 0.02046, Accuracy = 99.41%: 100%|██████████| 572/572 [00:06<00:00, 84.54it/s]\n","[32 / 50]   Val: Loss = 0.11474, Accuracy = 96.92%: 100%|██████████| 13/13 [00:00<00:00, 81.12it/s]\n","[33 / 50] Train: Loss = 0.01951, Accuracy = 99.44%: 100%|██████████| 572/572 [00:06<00:00, 84.56it/s]\n","[33 / 50]   Val: Loss = 0.11661, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 78.39it/s]\n","[34 / 50] Train: Loss = 0.01846, Accuracy = 99.48%: 100%|██████████| 572/572 [00:06<00:00, 81.55it/s]\n","[34 / 50]   Val: Loss = 0.11907, Accuracy = 96.87%: 100%|██████████| 13/13 [00:00<00:00, 82.48it/s]\n","[35 / 50] Train: Loss = 0.01773, Accuracy = 99.50%: 100%|██████████| 572/572 [00:06<00:00, 85.81it/s] \n","[35 / 50]   Val: Loss = 0.12135, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 81.79it/s]\n","[36 / 50] Train: Loss = 0.01663, Accuracy = 99.55%: 100%|██████████| 572/572 [00:06<00:00, 83.75it/s]\n","[36 / 50]   Val: Loss = 0.12229, Accuracy = 96.83%: 100%|██████████| 13/13 [00:00<00:00, 77.84it/s]\n","[37 / 50] Train: Loss = 0.01598, Accuracy = 99.57%: 100%|██████████| 572/572 [00:06<00:00, 83.82it/s]\n","[37 / 50]   Val: Loss = 0.12491, Accuracy = 96.82%: 100%|██████████| 13/13 [00:00<00:00, 85.52it/s]\n","[38 / 50] Train: Loss = 0.01514, Accuracy = 99.60%: 100%|██████████| 572/572 [00:06<00:00, 83.73it/s]\n","[38 / 50]   Val: Loss = 0.12902, Accuracy = 96.84%: 100%|██████████| 13/13 [00:00<00:00, 81.31it/s]\n","[39 / 50] Train: Loss = 0.01429, Accuracy = 99.63%: 100%|██████████| 572/572 [00:06<00:00, 84.09it/s]\n","[39 / 50]   Val: Loss = 0.13066, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 79.57it/s]\n","[40 / 50] Train: Loss = 0.01355, Accuracy = 99.65%: 100%|██████████| 572/572 [00:06<00:00, 84.05it/s]\n","[40 / 50]   Val: Loss = 0.13175, Accuracy = 96.78%: 100%|██████████| 13/13 [00:00<00:00, 76.38it/s]\n","[41 / 50] Train: Loss = 0.01289, Accuracy = 99.67%: 100%|██████████| 572/572 [00:06<00:00, 83.03it/s]\n","[41 / 50]   Val: Loss = 0.13757, Accuracy = 96.73%: 100%|██████████| 13/13 [00:00<00:00, 79.04it/s]\n","[42 / 50] Train: Loss = 0.01213, Accuracy = 99.70%: 100%|██████████| 572/572 [00:06<00:00, 83.12it/s] \n","[42 / 50]   Val: Loss = 0.13925, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 75.75it/s]\n","[43 / 50] Train: Loss = 0.01158, Accuracy = 99.72%: 100%|██████████| 572/572 [00:06<00:00, 84.02it/s]\n","[43 / 50]   Val: Loss = 0.13978, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 80.38it/s]\n","[44 / 50] Train: Loss = 0.01107, Accuracy = 99.73%: 100%|██████████| 572/572 [00:06<00:00, 84.18it/s]\n","[44 / 50]   Val: Loss = 0.14358, Accuracy = 96.74%: 100%|██████████| 13/13 [00:00<00:00, 76.68it/s]\n","[45 / 50] Train: Loss = 0.01026, Accuracy = 99.76%: 100%|██████████| 572/572 [00:06<00:00, 83.76it/s]\n","[45 / 50]   Val: Loss = 0.14388, Accuracy = 96.66%: 100%|██████████| 13/13 [00:00<00:00, 77.12it/s]\n","[46 / 50] Train: Loss = 0.00975, Accuracy = 99.78%: 100%|██████████| 572/572 [00:06<00:00, 83.94it/s]\n","[46 / 50]   Val: Loss = 0.14710, Accuracy = 96.72%: 100%|██████████| 13/13 [00:00<00:00, 80.56it/s]\n","[47 / 50] Train: Loss = 0.00919, Accuracy = 99.80%: 100%|██████████| 572/572 [00:06<00:00, 83.84it/s]\n","[47 / 50]   Val: Loss = 0.15097, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 81.06it/s]\n","[48 / 50] Train: Loss = 0.00868, Accuracy = 99.81%: 100%|██████████| 572/572 [00:06<00:00, 84.13it/s]\n","[48 / 50]   Val: Loss = 0.15627, Accuracy = 96.69%: 100%|██████████| 13/13 [00:00<00:00, 76.15it/s]\n","[49 / 50] Train: Loss = 0.00827, Accuracy = 99.82%: 100%|██████████| 572/572 [00:06<00:00, 83.42it/s]\n","[49 / 50]   Val: Loss = 0.15549, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 82.27it/s]\n","[50 / 50] Train: Loss = 0.00776, Accuracy = 99.85%: 100%|██████████| 572/572 [00:06<00:00, 85.11it/s]\n","[50 / 50]   Val: Loss = 0.16110, Accuracy = 96.67%: 100%|██████████| 13/13 [00:00<00:00, 77.11it/s]\n"],"name":"stderr"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"2Ne_8f24h8kg"},"source":["**Задание** Оцените качество модели на тестовой выборке. Обратите внимание, вовсе не обязательно ограничиваться векторами из урезанной матрицы - вполне могут найтись слова в тесте, которых не было в трейне и для которых есть эмбеддинги.\n","\n","Добейтесь качества лучше прошлых моделей."]},{"cell_type":"code","metadata":{"colab_type":"code","id":"HPUuAPGhEGVR","outputId":"ed9fbf1a-ca3a-4f64-ac61-a4e5eb042873","executionInfo":{"status":"ok","timestamp":1576317551061,"user_tz":-180,"elapsed":878,"user":{"displayName":"Alexandr Frolov","photoUrl":"","userId":"16154276962738693262"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["batch_size = len(X_test)\n","name = 'Test'\n","epoch_loss = 0\n","correct_count = 0\n","sum_count = 0\n","model.eval()\n","batches_count = math.ceil(len(X_test) / batch_size)\n","with tqdm(total=batches_count) as progress_bar:\n","  for i, (X_batch, y_batch) in enumerate(iterate_batches((X_test, y_test), batch_size)):\n","    X_batch, y_batch = LongTensor(X_batch), LongTensor(y_batch)\n","\n","    logits = model(X_batch)\n","\n","    indices = torch.max(logits, dim=2)[1]\n","\n","    loss = criterion(logits.view(-1, 13), y_batch.view(-1))\n","\n","    epoch_loss += loss.item()\n","\n","  #   if optimizer:\n","  #       optimizer.zero_grad()\n","  #       loss.backward()\n","  #       optimizer.step()\n","\n","    indices = torch.max(logits, dim=2)[1]   \n","    correct_samples = float(torch.sum(indices[y_batch!=0] == y_batch[y_batch!=0]))\n","\n","    cur_correct_count, cur_sum_count = correct_samples, y_batch[y_batch!=0].shape[0]\n","\n","    correct_count += cur_correct_count\n","    sum_count += cur_sum_count\n","\n","    progress_bar.update()\n","    progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n","        name, loss.item(), cur_correct_count / cur_sum_count)\n","                                )\n","progress_bar.set_description('{:>5s} Loss = {:.5f}, Accuracy = {:.2%}'.format(\n","    name, epoch_loss / batches_count, correct_count / sum_count))"],"execution_count":0,"outputs":[{"output_type":"stream","text":[" Test Loss = 0.16628, Accuracy = 96.65%: 100%|██████████| 1/1 [00:00<00:00,  3.48it/s]\n"],"name":"stderr"}]}]}