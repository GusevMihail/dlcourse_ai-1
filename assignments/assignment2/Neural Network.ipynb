{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задание 2.1 - Нейронные сети\n",
    "\n",
    "В этом задании вы реализуете и натренируете настоящую нейроную сеть своими руками!\n",
    "\n",
    "В некотором смысле это будет расширением прошлого задания - нам нужно просто составить несколько линейных классификаторов вместе!\n",
    "\n",
    "<img src=\"https://i.redd.it/n9fgba8b0qr01.png\" alt=\"Stack_more_layers\" width=\"400px\"/>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n  %reload_ext autoreload\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from assignment2.dataset import load_svhn, random_split_train_val\n",
    "from assignment2.gradient_check import check_layer_gradient, check_layer_param_gradient, check_model_gradient\n",
    "from assignment2.layers import FullyConnectedLayer, ReLULayer\n",
    "from assignment2.model import TwoLayerNet\n",
    "from assignment2.trainer import Trainer, Dataset \n",
    "from assignment2.optim import SGD, MomentumSGD\n",
    "from assignment2.metrics import multiclass_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Загружаем данные\n",
    "\n",
    "И разделяем их на training и validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "def prepare_for_neural_network(train_X, test_X):\n",
    "    train_flat = train_X.reshape(train_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    test_flat = test_X.reshape(test_X.shape[0], -1).astype(np.float) / 255.0\n",
    "    \n",
    "    # Subtract mean\n",
    "    mean_image = np.mean(train_flat, axis = 0)\n",
    "    train_flat -= mean_image\n",
    "    test_flat -= mean_image\n",
    "    \n",
    "    return train_flat, test_flat\n",
    "    \n",
    "train_X, train_y, test_X, test_y = load_svhn(\"data\", max_train=10000, max_test=1000)    \n",
    "train_X, test_X = prepare_for_neural_network(train_X, test_X)\n",
    "# Split train into train and val\n",
    "train_X, train_y, val_X, val_y = random_split_train_val(train_X, train_y, num_val = 1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как всегда, начинаем с кирпичиков\n",
    "\n",
    "Мы будем реализовывать необходимые нам слои по очереди. Каждый слой должен реализовать:\n",
    "- прямой проход (forward pass), который генерирует выход слоя по входу и запоминает необходимые данные\n",
    "- обратный проход (backward pass), который получает градиент по выходу слоя и вычисляет градиент по входу и по параметрам\n",
    "\n",
    "Начнем с ReLU, у которого параметров нет."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "run batch version of gradient check\n  analytic_grad\n [[ 1.61599406  0.          2.76252524]\n [-0.         -0.5060347  -0.94302519]] \n\n  numeric_grad\n [[ 1.61599406  0.          2.76252524]\n [ 0.         -0.5060347  -0.94302519]] \n\nGradient check passed!\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# TODO: Implement ReLULayer layer in layers.py\n",
    "# Note: you'll need to copy implementation of the gradient_check function from the previous assignment\n",
    "\n",
    "X = np.array([[1,-2,3],\n",
    "              [-1, 2, 0.1]\n",
    "              ])\n",
    "\n",
    "assert check_layer_gradient(ReLULayer(), X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь реализуем полносвязный слой (fully connected layer), у которого будет два массива параметров: W (weights) и B (bias).\n",
    "\n",
    "Все параметры наши слои будут использовать для параметров специальный класс `Param`, в котором будут храниться значения параметров и градиенты этих параметров, вычисляемые во время обратного прохода.\n",
    "\n",
    "Это даст возможность аккумулировать (суммировать) градиенты из разных частей функции потерь, например, из cross-entropy loss и regularization loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "run batch version of gradient check\n  analytic_grad\n [[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]] \n\n  numeric_grad\n [[ 4.18948095e-04  1.76337767e-03 -3.49979411e-04]\n [ 7.82747412e-05 -6.12293143e-04 -1.09946675e-03]] \n\nGradient check passed!\n\nrun batch version of gradient check\n  analytic_grad\n [[-1.67429365  2.15318718 -0.15893916 -0.56958173]\n [ 3.34858731 -4.30637437  0.31787831  1.13916345]\n [-0.36258122  5.45842018 -3.37773459 -0.89339967]] \n\n  numeric_grad\n [[-1.67429365  2.15318718 -0.15893916 -0.56958173]\n [ 3.34858731 -4.30637437  0.31787831  1.13916345]\n [-0.36258122  5.45842018 -3.37773459 -0.89339967]] \n\nGradient check passed!\n\nrun batch version of gradient check\n  analytic_grad\n [[-0.632844   -0.39002832  1.75340995  0.51081366]] \n\n  numeric_grad\n [[-0.632844   -0.39002832  1.75340995  0.51081366]] \n\nGradient check passed!\n\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# TODO: Implement FullyConnected layer forward and backward methods\n",
    "assert check_layer_gradient(FullyConnectedLayer(3, 4), X)\n",
    "# TODO: Implement storing gradients for W and B\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'W')\n",
    "assert check_layer_param_gradient(FullyConnectedLayer(3, 4), X, 'B')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Создаем нейронную сеть\n",
    "\n",
    "Теперь мы реализуем простейшую нейронную сеть с двумя полносвязным слоями и нелинейностью ReLU. Реализуйте функцию `compute_loss_and_gradients`, она должна запустить прямой и обратный проход через оба слоя для вычисления градиентов.\n",
    "\n",
    "Не забудьте реализовать очистку градиентов в начале функции."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Checking gradient for fc1_W\nrun batch version of gradient check\n",
      "  analytic_grad\n [[ 1.05393580e-05  0.00000000e+00 -2.23528083e-05]\n [ 1.25578983e-05  0.00000000e+00 -4.99938161e-06]\n [ 3.57140937e-05  0.00000000e+00  4.39014438e-05]\n ...\n [-9.80687160e-05  0.00000000e+00 -8.39833186e-05]\n [-8.76305797e-05  0.00000000e+00 -5.28532904e-05]\n [-6.98006962e-05  0.00000000e+00 -2.94876473e-06]] \n\n  numeric_grad\n [[ 1.05393694e-05  0.00000000e+00 -2.23528085e-05]\n [ 1.25578881e-05  0.00000000e+00 -4.99937869e-06]\n [ 3.57140983e-05  0.00000000e+00  4.39014158e-05]\n ...\n [-9.80687087e-05  0.00000000e+00 -8.39833092e-05]\n [-8.76305917e-05  0.00000000e+00 -5.28532773e-05]\n [-6.98006986e-05  0.00000000e+00 -2.94875235e-06]] \n\nGradient check passed!\n\nChecking gradient for fc1_B\nrun batch version of gradient check\n  analytic_grad\n [[0.00065346 0.         0.0006072 ]] \n\n  numeric_grad\n [[0.00065346 0.         0.0006072 ]] \n\nGradient check passed!\n\nChecking gradient for fc2_W\nrun batch version of gradient check\n  analytic_grad\n [[ 0.00037503  0.00037535  0.00037567  0.0003749   0.0003757   0.00037455\n   0.00037548  0.00037493  0.00037505 -0.00337666]\n [ 0.          0.          0.          0.          0.          0.\n   0.          0.          0.          0.        ]\n [ 0.00050123  0.00050165  0.00050209  0.00050104  0.00050212  0.00050057\n   0.00050182  0.00050108  0.00050126 -0.00451284]] \n\n  numeric_grad\n [[ 0.00037503  0.00037535  0.00037567  0.0003749   0.00037571  0.00037455\n   0.00037548  0.00037493  0.00037505 -0.00337666]\n [ 0.          0.          0.          0.          0.          0.\n   0.          0.          0.          0.        ]\n [ 0.00050123  0.00050165  0.00050209  0.00050104  0.00050212  0.00050057\n   0.00050182  0.00050108  0.00050126 -0.00451284]] \n\nGradient check passed!\n\nChecking gradient for fc2_B\nrun batch version of gradient check\n  analytic_grad\n [[ 0.09993815  0.10002254  0.10010892  0.09990119  0.10011637  0.09980764\n   0.1000566   0.09990923  0.09994333 -0.89980397]] \n\n  numeric_grad\n [[ 0.09993815  0.10002254  0.10010892  0.09990119  0.10011637  0.09980764\n   0.1000566   0.09990923  0.09994333 -0.89980397]] \n\nGradient check passed!\n\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 9
    }
   ],
   "source": [
    "# TODO: In model.py, implement compute_loss_and_gradients function\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 0)\n",
    "loss = model.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "\n",
    "# TODO Now implement backward pass and aggregate all of the params\n",
    "check_model_gradient(model, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавьте к модели регуляризацию - она должна прибавляться к loss и делать свой вклад в градиенты."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Checking gradient for fc1_W\nrun batch version of gradient check\n",
      "  analytic_grad\n [[-0.01976325 -0.00117834 -0.02135352]\n [ 0.01538378  0.0081576   0.01097304]\n [ 0.02261739 -0.00670261  0.02142476]\n ...\n [-0.00958147  0.00678107 -0.00191968]\n [ 0.03099914 -0.00714554  0.00908816]\n [ 0.02173933  0.0131764   0.01097147]] \n\n  numeric_grad\n [[-0.01976325 -0.00117834 -0.02135352]\n [ 0.01538378  0.0081576   0.01097304]\n [ 0.02261739 -0.00670261  0.02142476]\n ...\n [-0.00958147  0.00678107 -0.00191968]\n [ 0.03099914 -0.00714554  0.00908816]\n [ 0.02173933  0.0131764   0.01097147]] \n\nGradient check passed!\n\nChecking gradient for fc1_B\nrun batch version of gradient check\n  analytic_grad\n [[-0.01013572  0.0029346   0.00803137]] \n\n  numeric_grad\n [[-0.01013572  0.0029346   0.00803137]] \n\nGradient check passed!\n\nChecking gradient for fc2_W\nrun batch version of gradient check\n  analytic_grad\n [[ 0.00339013  0.02963675 -0.00331576  0.00066842  0.05166266  0.01660512\n  -0.01936271  0.00995913  0.01586149 -0.01602389]\n [-0.00517349 -0.02297789  0.00672213 -0.03158611 -0.01491909 -0.02681439\n   0.0074182  -0.00592797  0.00879223 -0.04433188]\n [-0.01543066  0.01599222 -0.02280474  0.02064731 -0.00284956 -0.01654026\n  -0.01526608  0.00448106  0.04409116  0.05259885]] \n\n  numeric_grad\n [[ 0.00339013  0.02963675 -0.00331576  0.00066842  0.05166266  0.01660512\n  -0.01936271  0.00995913  0.01586149 -0.01602389]\n [-0.00517349 -0.02297789  0.00672213 -0.03158611 -0.01491909 -0.02681439\n   0.0074182  -0.00592797  0.00879223 -0.04433188]\n [-0.01543065  0.01599222 -0.02280474  0.02064731 -0.00284956 -0.01654026\n  -0.01526608  0.00448106  0.04409116  0.05259885]] \n\nGradient check passed!\n\nChecking gradient for fc2_B\nrun batch version of gradient check\n  analytic_grad\n [[ 0.07316861  0.11352815  0.13350763  0.07497746  0.08745139  0.10584878\n   0.10082955  0.07217977  0.11503897 -0.89125851]] \n\n  numeric_grad\n [[ 0.07316861  0.11352815  0.13350763  0.07497746  0.08745139  0.10584878\n   0.10082955  0.07217977  0.11503897 -0.89125851]] \n\nGradient check passed!\n\n"
     ],
     "output_type": "stream"
    },
    {
     "data": {
      "text/plain": "True"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 10
    }
   ],
   "source": [
    "# TODO Now implement l2 regularization in the forward and backward pass\n",
    "model_with_reg = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 3, reg = 1e1)\n",
    "loss_with_reg = model_with_reg.compute_loss_and_gradients(train_X[:2], train_y[:2])\n",
    "assert loss_with_reg > loss and not np.isclose(loss_with_reg, loss), \\\n",
    "    \"Loss with regularization (%2.4f) should be higher than without it (%2.4f)!\" % (loss, loss_with_reg)\n",
    "\n",
    "check_model_gradient(model_with_reg, train_X[:2], train_y[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Также реализуем функцию предсказания (вычисления значения) модели на новых данных.\n",
    "\n",
    "Какое значение точности мы ожидаем увидеть до начала тренировки?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0.1"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 11
    }
   ],
   "source": [
    "# Finally, implement predict function!\n",
    "\n",
    "# TODO: Implement predict function\n",
    "# What would be the value we expect?\n",
    "multiclass_accuracy(model_with_reg.predict(train_X[:30]), train_y[:30]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Допишем код для процесса тренировки\n",
    "\n",
    "Если все реализовано корректно, значение функции ошибки должно уменьшаться с каждой эпохой, пусть и медленно. Не беспокойтесь пока про validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loss: 1.973762, Train accuracy: 0.230889, val accuracy: 0.252000, lr: 0.100000\n",
      "Loss: 1.751880, Train accuracy: 0.428667, val accuracy: 0.434000, lr: 0.100000\n",
      "Loss: 1.161730, Train accuracy: 0.514444, val accuracy: 0.530000, lr: 0.100000\n",
      "Loss: 1.198346, Train accuracy: 0.604667, val accuracy: 0.587000, lr: 0.100000\n",
      "Loss: 1.090121, Train accuracy: 0.671111, val accuracy: 0.651000, lr: 0.100000\n",
      "Loss: 0.471595, Train accuracy: 0.710889, val accuracy: 0.680000, lr: 0.100000\n",
      "Loss: 0.737977, Train accuracy: 0.713111, val accuracy: 0.682000, lr: 0.100000\n",
      "Loss: 0.547223, Train accuracy: 0.747222, val accuracy: 0.689000, lr: 0.100000\n",
      "Loss: 1.080923, Train accuracy: 0.738556, val accuracy: 0.706000, lr: 0.100000\n",
      "Loss: 0.420400, Train accuracy: 0.755222, val accuracy: 0.704000, lr: 0.100000\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, \n",
    "                    hidden_layer_size = 100, reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate = 1e-1, num_epochs=10)\n",
    "\n",
    "# TODO Implement missing pieces in Trainer.fit function\n",
    "# You should expect loss to go down every epoch, even if it's slow\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "[<matplotlib.lines.Line2D at 0x1d4c6f06ba8>]"
     },
     "metadata": {},
     "output_type": "execute_result",
     "execution_count": 13
    },
    {
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Улучшаем процесс тренировки\n",
    "\n",
    "Мы реализуем несколько ключевых оптимизаций, необходимых для тренировки современных нейросетей."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Уменьшение скорости обучения (learning rate decay)\n",
    "\n",
    "Одна из необходимых оптимизаций во время тренировки нейронных сетей - постепенное уменьшение скорости обучения по мере тренировки.\n",
    "\n",
    "Один из стандартных методов - уменьшение скорости обучения (learning rate) каждые N эпох на коэффициент d (часто называемый decay). Значения N и d, как всегда, являются гиперпараметрами и должны подбираться на основе эффективности на проверочных данных (validation data). \n",
    "\n",
    "В нашем случае N будет равным 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loss: 2.167917, Train accuracy: 0.220778, val accuracy: 0.236000, lr: 0.099000\n",
      "Loss: 2.058752, Train accuracy: 0.398222, val accuracy: 0.393000, lr: 0.098010\n",
      "Loss: 0.976473, Train accuracy: 0.540778, val accuracy: 0.553000, lr: 0.097030\n",
      "Loss: 1.270165, Train accuracy: 0.614333, val accuracy: 0.607000, lr: 0.096060\n",
      "Loss: 1.090307, Train accuracy: 0.636667, val accuracy: 0.636000, lr: 0.095099\n",
      "Loss: 1.108789, Train accuracy: 0.673778, val accuracy: 0.644000, lr: 0.094148\n",
      "Loss: 0.785766, Train accuracy: 0.721889, val accuracy: 0.695000, lr: 0.093207\n",
      "Loss: 1.127368, Train accuracy: 0.721556, val accuracy: 0.667000, lr: 0.092274\n",
      "Loss: 0.878017, Train accuracy: 0.745556, val accuracy: 0.700000, lr: 0.091352\n",
      "Loss: 1.079874, Train accuracy: 0.765778, val accuracy: 0.693000, lr: 0.090438\n",
      "Loss: 1.293783, Train accuracy: 0.768222, val accuracy: 0.690000, lr: 0.089534\n",
      "Loss: 0.837942, Train accuracy: 0.784889, val accuracy: 0.702000, lr: 0.088638\n",
      "Loss: 0.681580, Train accuracy: 0.817444, val accuracy: 0.727000, lr: 0.087752\n",
      "Loss: 0.526593, Train accuracy: 0.815556, val accuracy: 0.727000, lr: 0.086875\n",
      "Loss: 0.368418, Train accuracy: 0.838000, val accuracy: 0.724000, lr: 0.086006\n",
      "Loss: 0.520899, Train accuracy: 0.846889, val accuracy: 0.744000, lr: 0.085146\n",
      "Loss: 0.617730, Train accuracy: 0.843778, val accuracy: 0.748000, lr: 0.084294\n",
      "Loss: 0.523522, Train accuracy: 0.838111, val accuracy: 0.720000, lr: 0.083451\n",
      "Loss: 0.494579, Train accuracy: 0.848889, val accuracy: 0.735000, lr: 0.082617\n",
      "Loss: 0.562322, Train accuracy: 0.844111, val accuracy: 0.722000, lr: 0.081791\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# TODO Implement learning rate decay inside Trainer.fit method\n",
    "# Decay should happen once per epoch\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, \n",
    "                    reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, \n",
    "                  learning_rate_decay=0.99)\n",
    "\n",
    "initial_learning_rate = trainer.learning_rate\n",
    "loss_history, train_history, val_history = trainer.fit()\n",
    "\n",
    "assert trainer.learning_rate < initial_learning_rate, \"Learning rate should've been reduced\"\n",
    "assert trainer.learning_rate > 0.5*initial_learning_rate, \"Learning rate shouldn'tve been reduced that much!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Накопление импульса (Momentum SGD)\n",
    "\n",
    "Другой большой класс оптимизаций - использование более эффективных методов градиентного спуска. Мы реализуем один из них - накопление импульса (Momentum SGD).\n",
    "\n",
    "Этот метод хранит скорость движения, использует градиент для ее изменения на каждом шаге, и изменяет веса пропорционально значению скорости.\n",
    "(Физическая аналогия: Вместо скорости градиенты теперь будут задавать ускорение, но будет присутствовать сила трения.)\n",
    "\n",
    "```\n",
    "velocity = momentum * velocity - learning_rate * gradient \n",
    "w = w + velocity\n",
    "```\n",
    "\n",
    "`momentum` здесь коэффициент затухания, который тоже является гиперпараметром (к счастью, для него часто есть хорошее значение по умолчанию, типичный диапазон -- 0.8-0.99).\n",
    "\n",
    "Несколько полезных ссылок, где метод разбирается более подробно:  \n",
    "http://cs231n.github.io/neural-networks-3/#sgd  \n",
    "https://distill.pub/2017/momentum/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loss: 2.092324, Train accuracy: 0.222556, val accuracy: 0.229000, lr: 0.099000\n",
      "Loss: 1.860824, Train accuracy: 0.379556, val accuracy: 0.389000, lr: 0.098010\n",
      "Loss: 1.590450, Train accuracy: 0.505556, val accuracy: 0.517000, lr: 0.097030\n",
      "Loss: 1.847590, Train accuracy: 0.636556, val accuracy: 0.627000, lr: 0.096060\n",
      "Loss: 1.432718, Train accuracy: 0.672444, val accuracy: 0.667000, lr: 0.095099\n",
      "Loss: 1.785398, Train accuracy: 0.718222, val accuracy: 0.693000, lr: 0.094148\n",
      "Loss: 0.947681, Train accuracy: 0.698778, val accuracy: 0.670000, lr: 0.093207\n",
      "Loss: 1.096071, Train accuracy: 0.721889, val accuracy: 0.676000, lr: 0.092274\n",
      "Loss: 0.648033, Train accuracy: 0.732556, val accuracy: 0.688000, lr: 0.091352\n",
      "Loss: 0.707297, Train accuracy: 0.747889, val accuracy: 0.704000, lr: 0.090438\n",
      "Loss: 0.667226, Train accuracy: 0.753556, val accuracy: 0.693000, lr: 0.089534\n",
      "Loss: 0.513031, Train accuracy: 0.778556, val accuracy: 0.712000, lr: 0.088638\n",
      "Loss: 0.717841, Train accuracy: 0.807667, val accuracy: 0.742000, lr: 0.087752\n",
      "Loss: 0.550896, Train accuracy: 0.799667, val accuracy: 0.721000, lr: 0.086875\n",
      "Loss: 1.029893, Train accuracy: 0.819667, val accuracy: 0.714000, lr: 0.086006\n",
      "Loss: 0.673848, Train accuracy: 0.836778, val accuracy: 0.744000, lr: 0.085146\n",
      "Loss: 0.588906, Train accuracy: 0.841222, val accuracy: 0.739000, lr: 0.084294\n",
      "Loss: 0.500806, Train accuracy: 0.822444, val accuracy: 0.724000, lr: 0.083451\n",
      "Loss: 0.589618, Train accuracy: 0.850889, val accuracy: 0.730000, lr: 0.082617\n",
      "Loss: 0.521985, Train accuracy: 0.864444, val accuracy: 0.740000, lr: 0.081791\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# TODO: Implement MomentumSGD.update function in optim.py\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, \n",
    "                    reg = 1e-5)\n",
    "dataset = Dataset(train_X, train_y, val_X, val_y)\n",
    "trainer = Trainer(model, dataset, MomentumSGD(), \n",
    "                  learning_rate=1e-1, learning_rate_decay=0.99)\n",
    "\n",
    "# You should see even better results than before!\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ну что, давайте уже тренировать сеть!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Последний тест - переобучимся (overfit) на маленьком наборе данных\n",
    "\n",
    "Хороший способ проверить, все ли реализовано корректно - переобучить сеть на маленьком наборе данных.  \n",
    "Наша модель обладает достаточной мощностью, чтобы приблизить маленький набор данных идеально, поэтому мы ожидаем, что на нем мы быстро дойдем до 100% точности на тренировочном наборе. \n",
    "\n",
    "Если этого не происходит, то где-то была допущена ошибка!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loss: 2.343311, Train accuracy: 0.200000, val accuracy: 0.133333, lr: 0.100000\nLoss: 2.319494, Train accuracy: 0.200000, val accuracy: 0.133333, lr: 0.100000\n",
      "Loss: 2.344415, Train accuracy: 0.200000, val accuracy: 0.133333, lr: 0.100000\nLoss: 2.301831, Train accuracy: 0.200000, val accuracy: 0.133333, lr: 0.100000\nLoss: 2.282484, Train accuracy: 0.200000, val accuracy: 0.133333, lr: 0.100000\nLoss: 2.227485, Train accuracy: 0.200000, val accuracy: 0.066667, lr: 0.100000\nLoss: 2.247323, Train accuracy: 0.200000, val accuracy: 0.066667, lr: 0.100000\nLoss: 2.170390, Train accuracy: 0.200000, val accuracy: 0.133333, lr: 0.100000",
      "\nLoss: 2.228315, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 2.353823, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 2.006564, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000",
      "\nLoss: 2.127238, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.608926, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 2.047132, Train accuracy: 0.333333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.660990, Train accuracy: 0.333333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.851157, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\n",
      "Loss: 2.454232, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.470944, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.821988, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\n",
      "Loss: 2.178785, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.691652, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.464918, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.582852, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.925990, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.773455, Train accuracy: 0.400000, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.234214, Train accuracy: 0.466667, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.503882, Train accuracy: 0.466667, val accuracy: 0.066667, lr: 0.100000\n",
      "Loss: 1.187115, Train accuracy: 0.466667, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.462689, Train accuracy: 0.466667, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.942258, Train accuracy: 0.466667, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.949574, Train accuracy: 0.466667, val accuracy: 0.066667, lr: 0.100000\n",
      "Loss: 1.643039, Train accuracy: 0.533333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.771681, Train accuracy: 0.600000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.759061, Train accuracy: 0.600000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.887799, Train accuracy: 0.600000, val accuracy: 0.066667, lr: 0.100000\nLoss: 2.179964, Train accuracy: 0.533333, val accuracy: 0.000000, lr: 0.100000\nLoss: 2.020796, Train accuracy: 0.600000, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.090921, Train accuracy: 0.600000, val accuracy: 0.066667, lr: 0.100000",
      "\nLoss: 1.976898, Train accuracy: 0.666667, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.449197, Train accuracy: 0.666667, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.297950, Train accuracy: 0.666667, val accuracy: 0.066667, lr: 0.100000",
      "\nLoss: 1.542805, Train accuracy: 0.666667, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.171500, Train accuracy: 0.666667, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.637635, Train accuracy: 0.666667, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.986562, Train accuracy: 0.666667, val accuracy: 0.066667, lr: 0.100000\nLoss: 2.052029, Train accuracy: 0.666667, val accuracy: 0.066667, lr: 0.100000\n",
      "Loss: 1.054725, Train accuracy: 0.666667, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.886474, Train accuracy: 0.733333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.673229, Train accuracy: 0.733333, val accuracy: 0.133333, lr: 0.100000\nLoss: 1.156699, Train accuracy: 0.800000, val accuracy: 0.066667, lr: 0.100000\n",
      "Loss: 1.095188, Train accuracy: 0.733333, val accuracy: 0.133333, lr: 0.100000\nLoss: 1.516842, Train accuracy: 0.733333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.412652, Train accuracy: 0.733333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.458610, Train accuracy: 0.800000, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.413610, Train accuracy: 0.800000, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.948770, Train accuracy: 0.800000, val accuracy: 0.066667, lr: 0.100000\n",
      "Loss: 1.901573, Train accuracy: 0.800000, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.355174, Train accuracy: 0.733333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.791769, Train accuracy: 0.733333, val accuracy: 0.133333, lr: 0.100000",
      "\nLoss: 1.922676, Train accuracy: 0.733333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.656876, Train accuracy: 0.733333, val accuracy: 0.066667, lr: 0.100000\nLoss: 2.095059, Train accuracy: 0.800000, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.693867, Train accuracy: 0.800000, val accuracy: 0.066667, lr: 0.100000",
      "\nLoss: 1.658368, Train accuracy: 0.800000, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.710144, Train accuracy: 0.800000, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.984833, Train accuracy: 0.800000, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.694758, Train accuracy: 0.866667, val accuracy: 0.066667, lr: 0.100000\n",
      "Loss: 1.278300, Train accuracy: 0.866667, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.340980, Train accuracy: 0.866667, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.254693, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.557730, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.522803, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.661825, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000",
      "\nLoss: 1.678061, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.649513, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.442781, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.419692, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.280261, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\n",
      "Loss: 1.280827, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.508549, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.251723, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.367962, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.264583, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\n",
      "Loss: 1.827399, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.194376, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.517925, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\n",
      "Loss: 1.251765, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.546441, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.505875, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.310843, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.310200, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.470709, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\n",
      "Loss: 1.498062, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.170556, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.175903, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.277560, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000",
      "\nLoss: 1.305250, Train accuracy: 0.933333, val accuracy: 0.066667, lr: 0.100000\nLoss: 1.349195, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.284261, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.142537, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.118618, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\n",
      "Loss: 1.613488, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.283754, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.600273, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\n",
      "Loss: 1.222109, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.247810, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.274772, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.164365, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.269793, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.266279, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.492541, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000",
      "\nLoss: 1.278787, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.118925, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\n",
      "Loss: 1.292824, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.234724, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.159691, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.581165, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.222135, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.334998, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000",
      "\nLoss: 1.272473, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.584695, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.188293, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\n",
      "Loss: 1.286153, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.379700, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.176025, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.190443, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.336935, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.170954, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.617802, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000",
      "\nLoss: 1.278333, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.366067, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.352405, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000",
      "\nLoss: 1.500938, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.290282, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.180586, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.290144, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.102963, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.101868, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\n",
      "Loss: 1.152287, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.415761, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.287221, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.455727, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.357959, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.042405, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.305173, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.375391, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000",
      "\nLoss: 1.316192, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.349455, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.448131, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\nLoss: 1.341960, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.100000\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "data_size = 15\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, \n",
    "                    reg = 1e-1)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=1e-1, num_epochs=150, batch_size=5)\n",
    "\n",
    "# You should expect this to reach 1.0 training accuracy \n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь найдем гипепараметры, для которых этот процесс сходится быстрее.\n",
    "Если все реализовано корректно, то существуют параметры, при которых процесс сходится в **20** эпох или еще быстрее.\n",
    "Найдите их!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "text": [
      "Loss: 2.339336, Train accuracy: 0.200000, val accuracy: 0.133333, lr: 0.300000\nLoss: 2.278314, Train accuracy: 0.200000, val accuracy: 0.133333, lr: 0.300000\nLoss: 2.266186, Train accuracy: 0.200000, val accuracy: 0.133333, lr: 0.300000\nLoss: 2.322997, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.300000\nLoss: 2.072852, Train accuracy: 0.333333, val accuracy: 0.000000, lr: 0.300000\n",
      "Loss: 2.331758, Train accuracy: 0.333333, val accuracy: 0.066667, lr: 0.300000\nLoss: 1.979863, Train accuracy: 0.466667, val accuracy: 0.133333, lr: 0.300000\nLoss: 1.609449, Train accuracy: 0.466667, val accuracy: 0.000000, lr: 0.300000\nLoss: 1.739748, Train accuracy: 0.400000, val accuracy: 0.000000, lr: 0.300000\nLoss: 1.246179, Train accuracy: 0.400000, val accuracy: 0.066667, lr: 0.300000\nLoss: 1.246652, Train accuracy: 0.466667, val accuracy: 0.000000, lr: 0.300000\n",
      "Loss: 1.905688, Train accuracy: 0.466667, val accuracy: 0.066667, lr: 0.300000\nLoss: 1.475181, Train accuracy: 0.666667, val accuracy: 0.000000, lr: 0.300000\nLoss: 1.327172, Train accuracy: 0.600000, val accuracy: 0.066667, lr: 0.300000\n",
      "Loss: 1.270285, Train accuracy: 0.733333, val accuracy: 0.000000, lr: 0.300000\nLoss: 0.731440, Train accuracy: 0.866667, val accuracy: 0.000000, lr: 0.300000\nLoss: 0.624501, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.300000\nLoss: 0.473138, Train accuracy: 0.933333, val accuracy: 0.000000, lr: 0.300000\nLoss: 0.040159, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.300000\nLoss: 0.241249, Train accuracy: 1.000000, val accuracy: 0.000000, lr: 0.300000\n"
     ],
     "output_type": "stream"
    }
   ],
   "source": [
    "# Now, tweak some hyper parameters and make it train to 1.0 accuracy in 20 epochs or less\n",
    "\n",
    "model = TwoLayerNet(n_input = train_X.shape[1], n_output = 10, hidden_layer_size = 100, \n",
    "                    reg = 0)\n",
    "dataset = Dataset(train_X[:data_size], train_y[:data_size], val_X[:data_size], val_y[:data_size])\n",
    "# TODO: Change any hyperparamers or optimizators to reach training accuracy in 20 epochs\n",
    "trainer = Trainer(model, dataset, SGD(), learning_rate=3e-1, num_epochs=20, batch_size=5)\n",
    "\n",
    "loss_history, train_history, val_history = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Итак, основное мероприятие!\n",
    "\n",
    "Натренируйте лучшую нейросеть! Можно добавлять и изменять параметры, менять количество нейронов в слоях сети и как угодно экспериментировать. \n",
    "\n",
    "Добейтесь точности лучше **40%** на validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Let's train the best one-hidden-layer network we can\n",
    "\n",
    "learning_rates = 1e-1\n",
    "reg_strength = 1e-5\n",
    "learning_rate_decay = 0.99\n",
    "hidden_layer_size = 128\n",
    "num_epochs = 200\n",
    "batch_size = 64\n",
    "\n",
    "best_classifier = None\n",
    "best_val_accuracy = None\n",
    "\n",
    "loss_history = []\n",
    "train_history = []\n",
    "val_history = []\n",
    "\n",
    "# TODO find the best hyperparameters to train the network\n",
    "# Don't hesitate to add new values to the arrays above, perform experiments, use any tricks you want\n",
    "# You should expect to get to at least 40% of validation accuracy\n",
    "# Save loss/train/history of the best classifier to the variables above\n",
    "\n",
    "print('best validation accuracy achieved: %f' % best_val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 7))\n",
    "plt.subplot(211)\n",
    "plt.title(\"Loss\")\n",
    "plt.plot(loss_history)\n",
    "plt.subplot(212)\n",
    "plt.title(\"Train/validation accuracy\")\n",
    "plt.plot(train_history)\n",
    "plt.plot(val_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Как обычно, посмотрим, как наша лучшая модель работает на тестовых данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "test_pred = best_classifier.predict(test_X)\n",
    "test_accuracy = multiclass_accuracy(test_pred, test_y)\n",
    "print('Neural net test set accuracy: %f' % (test_accuracy, ))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "source": [],
    "metadata": {
     "collapsed": false
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}